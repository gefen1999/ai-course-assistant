{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-03T10:34:13.190503Z",
     "start_time": "2025-10-03T10:34:11.557984Z"
    }
   },
   "source": "pip install pinecone",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone in /home/student/.virtualenvs/final-project/lib/python3.10/site-packages (7.3.0)\r\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /home/student/.virtualenvs/final-project/lib/python3.10/site-packages (from pinecone) (2025.8.3)\r\n",
      "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /home/student/.virtualenvs/final-project/lib/python3.10/site-packages (from pinecone) (1.7.0)\r\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /home/student/.virtualenvs/final-project/lib/python3.10/site-packages (from pinecone) (0.0.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/student/.virtualenvs/final-project/lib/python3.10/site-packages (from pinecone) (2.9.0.post0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/student/.virtualenvs/final-project/lib/python3.10/site-packages (from pinecone) (4.14.1)\r\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/student/.virtualenvs/final-project/lib/python3.10/site-packages (from pinecone) (2.5.0)\r\n",
      "Requirement already satisfied: packaging<25.0,>=24.2 in /home/student/.virtualenvs/final-project/lib/python3.10/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /home/student/.virtualenvs/final-project/lib/python3.10/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.4)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/student/.virtualenvs/final-project/lib/python3.10/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.virtualenvs/final-project/lib/python3.10/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/student/.virtualenvs/final-project/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.1.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49m/home/student/.virtualenvs/final-project/bin/python3 -m pip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "import json\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import pdfplumber\n",
    "import docx\n",
    "from pdfminer.high_level import extract_text\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    SpacyTextSplitter\n",
    ")\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import spacy\n",
    "import subprocess\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')"
   ],
   "id": "954f6556c73c42ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T10:34:20.071747Z",
     "start_time": "2025-10-03T10:34:20.068103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ensure_spacy_model(model_name=\"en_core_web_sm\"):\n",
    "    \"\"\"Ensure that the given spaCy model is installed.\"\"\"\n",
    "    try:\n",
    "        spacy.load(model_name)\n",
    "    except OSError:\n",
    "        print(f\"⚠️ spaCy model '{model_name}' not found. Downloading...\")\n",
    "        subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", model_name], check=True)\n",
    "        print(f\"✅ Downloaded spaCy model: {model_name}\")"
   ],
   "id": "2950dc3c0819cbd5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T10:34:20.375455Z",
     "start_time": "2025-10-03T10:34:20.371960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def recursive_chunking(texts: list[str], chunk_size: int, overlap: int) -> list[str]:\n",
    "    char_size = chunk_size * AVG_WORD_LEN\n",
    "    char_overlap = overlap * AVG_WORD_LEN\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=char_size, chunk_overlap=char_overlap)\n",
    "    chunks = []\n",
    "    for text in texts:\n",
    "        chunks.extend(splitter.split_text(text))\n",
    "    return chunks"
   ],
   "id": "314469599e0ff1e6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T11:17:54.038176Z",
     "start_time": "2025-10-03T11:17:54.034048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def overlapping_chunking(texts: list[str], chunk_size: int, overlap: int) -> list[str]:\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        for i in range(0, len(words), step):\n",
    "            chunk = \" \".join(words[i:i + chunk_size])\n",
    "            if chunk:\n",
    "                chunks.append(chunk.strip())\n",
    "    return chunks"
   ],
   "id": "835968f47beb301e",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T11:17:50.951730Z",
     "start_time": "2025-10-03T11:17:50.947724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def spacy_chunking(texts: list[str], chunk_size: int, overlap: int) -> list[str]:\n",
    "    char_size = chunk_size * AVG_WORD_LEN\n",
    "    char_overlap = overlap * AVG_WORD_LEN\n",
    "    splitter = SpacyTextSplitter(chunk_size=char_size, chunk_overlap=char_overlap)\n",
    "    chunks = []\n",
    "    for text in texts:\n",
    "        chunks.extend(splitter.split_text(text))\n",
    "    return chunks"
   ],
   "id": "81e0aaf687a79b7d",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T10:34:21.198906Z",
     "start_time": "2025-10-03T10:34:21.196037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def select_chunking_strategy(strategy_name: str):\n",
    "    strategies = {\n",
    "        \"recursive\": recursive_chunking,\n",
    "        \"overlapping\": overlapping_chunking,\n",
    "        \"spacy\": spacy_chunking,\n",
    "    }\n",
    "    return strategies.get(strategy_name)"
   ],
   "id": "e35ae3500f0c0634",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T10:34:21.499011Z",
     "start_time": "2025-10-03T10:34:21.492466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_zip_file(zip_path: Path, extract_to: Path):\n",
    "    if not extract_to.exists():\n",
    "        extract_to.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(f\"Extracted {zip_path} to {extract_to}\")\n",
    "\n",
    "def remove_cid_artifacts(text):\n",
    "    return re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "\n",
    "def read_pdf(path: Path) -> str:\n",
    "    try:\n",
    "        text = extract_text(str(path))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {path.name}: {e}\")\n",
    "        text = \"\"\n",
    "    return remove_cid_artifacts(text)\n",
    "\n",
    "def read_docx(path: Path) -> str:\n",
    "    doc = docx.Document(str(path))\n",
    "    return \"\\n\".join(p.text for p in doc.paragraphs)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\.\\,\\;\\:\\?\\!\\-\\s]\", \" \", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def embed_chunks(texts: list[str], model: SentenceTransformer) -> np.ndarray:\n",
    "    if not texts:\n",
    "        return np.empty((0, model.get_sentence_embedding_dimension()), dtype=\"float32\")\n",
    "    embs = model.encode(texts, convert_to_numpy=True)\n",
    "    return normalize(embs, axis=1).astype(\"float32\")"
   ],
   "id": "9cfe297b4167371f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T10:34:21.660964Z",
     "start_time": "2025-10-03T10:34:21.655218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_index(pc, index_name, dimension, metric):\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=dimension,\n",
    "            metric = metric,\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "        )\n",
    "        print(f\"Created new Pinecone index: {index_name}\")\n",
    "    else:\n",
    "        print(f\"Using existing Pinecone index: {index_name}\")\n",
    "\n",
    "def save_to_pinecone(records, embeddings, pc, index_name, namespace=\"default\"):\n",
    "    index = pc.Index(index_name)\n",
    "    vectors = []\n",
    "    for i, record in enumerate(records):\n",
    "        vectors.append({\n",
    "            \"id\": f\"{Path(record['file']).stem}_chunk_{record['chunk_id']}\",\n",
    "            \"values\": embeddings[i].tolist(),\n",
    "            \"metadata\": {\n",
    "                \"file\": Path(record['file']).name,\n",
    "                \"chunk_id\": record['chunk_id'],\n",
    "                \"text\": record['text'],\n",
    "                \"length\": record['length'],\n",
    "            }\n",
    "        })\n",
    "    for i in range(0, len(vectors), 100):\n",
    "        index.upsert(vectors=vectors[i:i+100], namespace=namespace)\n",
    "    #print(f\"📌 Saved {len(vectors)} vectors to Pinecone namespace '{namespace}'.\")"
   ],
   "id": "57d2d1715b554fa3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T11:25:05.623875Z",
     "start_time": "2025-10-03T11:25:05.616641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_NAME = \"multi-qa-mpnet-base-dot-v1\"\n",
    "ZIP_PATH = Path(\"dataset.zip\")\n",
    "DATA_DIR = ZIP_PATH.with_suffix(\"\")\n",
    "WORDS_PER_CHUNK = 700\n",
    "WORDS_OVERLAP = 200\n",
    "AVG_WORD_LEN = 6\n",
    "PINECONE_INDEX_NAME = f'dotproduct-{WORDS_PER_CHUNK}'\n",
    "CHUNK_STRATEGIES = [\"spacy\", \"recursive\", \"overlapping\"]  # Options: recursive / overlapping / spacy\n",
    "\n",
    "with open(r\"src/api_keys.json\") as f:\n",
    "    api_keys = json.load(f)\n",
    "pc = Pinecone(api_key=api_keys[\"pinecone\"])"
   ],
   "id": "372fc45829c883ee",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loading model\n",
    "print(f\"[1/3] Loading model '{MODEL_NAME}'...\")\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "dimension = model.get_sentence_embedding_dimension()\n",
    "metric = PINECONE_INDEX_NAME.split(\"-\")[0]\n",
    "create_index(pc, PINECONE_INDEX_NAME, dimension, metric)"
   ],
   "id": "e642ef6bd7088d1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"[2/3] Extracting zip and reading documents...\")\n",
    "extract_zip_file(ZIP_PATH, DATA_DIR)\n",
    "raw_texts = []\n",
    "sources = []\n",
    "all_files = list(DATA_DIR.rglob(\"*\"))\n",
    "with tqdm(all_files, desc=\"Reading files\") as pbar:\n",
    "    for path in pbar:\n",
    "        if path.suffix.lower() == \".pdf\":\n",
    "            text = read_pdf(path)\n",
    "        elif path.suffix.lower() == \".docx\":\n",
    "            text = read_docx(path)\n",
    "        else:\n",
    "            continue\n",
    "        cleaned = clean_text(text)\n",
    "        raw_texts.append(cleaned)\n",
    "        sources.append(str(path))"
   ],
   "id": "1a4d3ab975948f93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"[3/3] Chunking, embedding and uploading text...\")\n",
    "for strategy in CHUNK_STRATEGIES:\n",
    "    if strategy == \"spacy\":\n",
    "      ensure_spacy_model(\"en_core_web_sm\")\n",
    "\n",
    "    chunk_func = select_chunking_strategy(strategy)\n",
    "    records = []\n",
    "    for file_path, text in zip(sources, raw_texts):\n",
    "        file_chunks = chunk_func([text], chunk_size=WORDS_PER_CHUNK, overlap=WORDS_OVERLAP)\n",
    "        file_name = Path(file_path).stem\n",
    "        for i, chunk in enumerate(file_chunks):\n",
    "            records.append({\n",
    "                \"file\": file_name,\n",
    "                \"chunk_id\": i,\n",
    "                \"text\": chunk,\n",
    "                \"length\": len(chunk.split()),\n",
    "            })\n",
    "\n",
    "    print(f\"{strategy} → Created {len(records)} chunks\")\n",
    "    embs = embed_chunks([r[\"text\"] for r in records], model)\n",
    "    namespace = strategy\n",
    "    save_to_pinecone(records, embs, pc, PINECONE_INDEX_NAME, namespace=namespace)\n",
    "    #print(f\"{strategy} → Uploaded {len(records)} chunks to Pinecone namespace '{namespace}'.\")\n"
   ],
   "id": "8773b7ea3df0386d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
